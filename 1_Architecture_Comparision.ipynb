{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rfmDqBxWV16"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWd-77PqWoAe"
      },
      "outputs": [],
      "source": [
        "# Define directories for train, test, and validation data\n",
        "train_dir = '/content/drive/MyDrive/MIS 548 Porject Dataset/Train/'\n",
        "test_dir = '/content/drive/MyDrive/MIS 548 Porject Dataset/Test/'\n",
        "val_dir = '/content/drive/MyDrive/MIS 548 Porject Dataset/Validation/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWaEYZzscAWD",
        "outputId": "5073eadc-47a9-4eec-b382-9b3bfb3408b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files in train directory:\n",
            "['Fake', 'Real']\n",
            "Files in test directory:\n",
            "['Real', 'Fake']\n",
            "Files in validation directory:\n",
            "['Fake', 'Real']\n"
          ]
        }
      ],
      "source": [
        "# List files in the train directory\n",
        "print(\"Files in train directory:\")\n",
        "print(os.listdir(train_dir))\n",
        "\n",
        "# List files in the test directory\n",
        "print(\"Files in test directory:\")\n",
        "print(os.listdir(test_dir))\n",
        "\n",
        "# List files in the validation directory\n",
        "print(\"Files in validation directory:\")\n",
        "print(os.listdir(val_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsrZf5EZcC21"
      },
      "outputs": [],
      "source": [
        "# Count the number of files in each subdirectory of the train directory\n",
        "train_real_files = len(os.listdir(os.path.join(train_dir, 'Real')))\n",
        "train_fake_files = len(os.listdir(os.path.join(train_dir, 'Fake')))\n",
        "\n",
        "# Count the number of files in each subdirectory of the train directory\n",
        "test_real_files = len(os.listdir(os.path.join(test_dir, 'Real')))\n",
        "test_fake_files = len(os.listdir(os.path.join(test_dir, 'Fake')))\n",
        "\n",
        "# Count the number of files in each subdirectory of the train directory\n",
        "val_real_files = len(os.listdir(os.path.join(val_dir, 'Real')))\n",
        "val_fake_files = len(os.listdir(os.path.join(val_dir, 'Fake')))\n",
        "\n",
        "# Print the counts\n",
        "print(\"Number of real images in train directory:\", train_real_files)\n",
        "print(\"Number of fake images in train directory:\", train_fake_files)\n",
        "\n",
        "# Print the counts\n",
        "print(\"Number of real images in train directory:\", test_real_files)\n",
        "print(\"Number of fake images in train directory:\", test_fake_files)\n",
        "\n",
        "# Print the counts\n",
        "print(\"Number of real images in train directory:\", val_real_files)\n",
        "print(\"Number of fake images in train directory:\", val_fake_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "NC8d2eg0WpJJ",
        "outputId": "b9baab7c-3db4-4702-8b81-13d80aa85273"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Sample larger than population or is negative",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f7b7b0a855d9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Randomly sample 5,000 images from each folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mrandbelow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
          ]
        }
      ],
      "source": [
        "# Randomly sample 5,000 images from each folder\n",
        "train_images = random.sample(os.listdir(train_dir), 10000)\n",
        "test_images = random.sample(os.listdir(test_dir), 2000)\n",
        "val_images = random.sample(os.listdir(val_dir), 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhmDSM6lWqg-"
      },
      "outputs": [],
      "source": [
        "# Define image dimensions and batch size\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8qIC9VIWxvP"
      },
      "outputs": [],
      "source": [
        "# Create data generators for train, test, and validation data\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=pd.DataFrame({\"filename\": train_images}),\n",
        "    directory=train_dir,\n",
        "    x_col=\"filename\",\n",
        "    y_col=None,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=pd.DataFrame({\"filename\": test_images}),\n",
        "    directory=test_dir,\n",
        "    x_col=\"filename\",\n",
        "    y_col=None,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    dataframe=pd.DataFrame({\"filename\": val_images}),\n",
        "    directory=val_dir,\n",
        "    x_col=\"filename\",\n",
        "    y_col=None,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh_cWJ2IWzQZ"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained ResNet50 model without the top classification layer\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLogYF7XW4_2"
      },
      "outputs": [],
      "source": [
        "# Freeze the convolutional layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNQxuKnPW6u0"
      },
      "outputs": [],
      "source": [
        "# Add custom classification layers on top of ResNet50\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTfkRMJPW8Qx"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNf5D0YbW9g9"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXEL79vYW_QP"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_images) // batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=len(val_images) // batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jij42T0mXBSs"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYdmKGaZtIox"
      },
      "source": [
        "分隔線"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rZKMQvbtKD7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9_hz_IttL3H"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "na0sti55tNIu"
      },
      "outputs": [],
      "source": [
        "# Path to the directory containing the dataset\n",
        "data_dir = \"/content/drive/MyDrive/MIS 548 Porject Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZouNe1otSxx"
      },
      "outputs": [],
      "source": [
        "# Data sampling and loading\n",
        "def sample_data(directory, sample_size):\n",
        "    files = os.listdir(directory)\n",
        "    sampled_files = random.sample(files, sample_size)\n",
        "    return [os.path.join(directory, file) for file in sampled_files]\n",
        "\n",
        "def load_data(directory, sample_size):\n",
        "    real_samples = sample_data(os.path.join(directory, \"Real\"), sample_size // 2)\n",
        "    fake_samples = sample_data(os.path.join(directory, \"Fake\"), sample_size // 2)\n",
        "    all_samples = real_samples + fake_samples\n",
        "    labels = [1] * (sample_size // 2) + [0] * (sample_size // 2)\n",
        "    all_samples, labels = shuffle(all_samples, labels)\n",
        "    return all_samples, labels\n",
        "\n",
        "train_samples, train_labels = load_data(os.path.join(data_dir, \"Train\"), 10000)\n",
        "val_samples, val_labels = load_data(os.path.join(data_dir, \"Validation\"), 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBv141QHh6B3",
        "outputId": "984032d0-2d04-41d2-8a24-4e82406a9851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llU3BRExta8r"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "def preprocess_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    return preprocess_image(image), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANZNHXintcyW"
      },
      "outputs": [],
      "source": [
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_samples, train_labels))\n",
        "train_dataset = train_dataset.map(load_and_preprocess_image)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_samples)).batch(32)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_samples, val_labels))\n",
        "val_dataset = val_dataset.map(load_and_preprocess_image)\n",
        "val_dataset = val_dataset.batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcG-53_8tgQL"
      },
      "outputs": [],
      "source": [
        "# Define the ResNet model architecture\n",
        "base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQsjDDsvtjS1"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwprsLsotmj1",
        "outputId": "5f5ca95d-56bc-42f1-9447-942f53db8278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_dataset,\n",
        "          epochs=10,\n",
        "          validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyhVd7Ottm-1",
        "outputId": "772c9e58-a4e7-4604-c311-2f73bc541ea3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save(\"resnet_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsnnyzu7tqrQ",
        "outputId": "4c34c850-b9eb-4d0c-e2ab-8ce5526fc30b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 189s 3s/step - loss: 0.9597 - accuracy: 0.7230\n",
            "Test Loss: 0.9596732258796692\n",
            "Test Accuracy: 0.7229999899864197\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "test_samples, test_labels = load_data(os.path.join(data_dir, \"Test\"), 2000)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_samples, test_labels))\n",
        "test_dataset = test_dataset.map(load_and_preprocess_image)\n",
        "test_dataset = test_dataset.batch(32)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lpjsMNWsZ49"
      },
      "source": [
        "分隔線 - ResNet 參數調整 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HRVCB2Csjl1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVz1Sd_Rsk-O"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1wA5b2UsnEE"
      },
      "outputs": [],
      "source": [
        "# Path to the directory containing the dataset\n",
        "data_dir = \"/content/drive/MyDrive/MIS 548 Porject Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97Wb1WaasoUK"
      },
      "outputs": [],
      "source": [
        "# Data sampling and loading\n",
        "def sample_data(directory, sample_size):\n",
        "    files = os.listdir(directory)\n",
        "    sampled_files = random.sample(files, sample_size)\n",
        "    return [os.path.join(directory, file) for file in sampled_files]\n",
        "\n",
        "def load_data(directory, sample_size):\n",
        "    real_samples = sample_data(os.path.join(directory, \"Real\"), sample_size // 2)\n",
        "    fake_samples = sample_data(os.path.join(directory, \"Fake\"), sample_size // 2)\n",
        "    all_samples = real_samples + fake_samples\n",
        "    labels = [1] * (sample_size // 2) + [0] * (sample_size // 2)\n",
        "    all_samples, labels = shuffle(all_samples, labels)\n",
        "    return all_samples, labels\n",
        "\n",
        "train_samples, train_labels = load_data(os.path.join(data_dir, \"Train\"), 10000)\n",
        "val_samples, val_labels = load_data(os.path.join(data_dir, \"Validation\"), 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxjsFxG5spzw",
        "outputId": "65e2a7e0-9327-4cc4-ee0e-2b1acd954bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 140110 images belonging to 2 classes.\n",
            "Found 39428 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing and augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.resnet.preprocess_input,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_dataset = train_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, \"Train\"),\n",
        "    #target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.resnet.preprocess_input\n",
        ")\n",
        "\n",
        "val_dataset = val_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, \"Validation\"),\n",
        "    #target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2vFuQCOs0Xo"
      },
      "outputs": [],
      "source": [
        "# Define the ResNet model architecture\n",
        "base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o67lIba8s3Fx"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XNuRhLxs44V"
      },
      "outputs": [],
      "source": [
        "# Compile the model with a custom learning rate scheduler\n",
        "initial_learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "lr_schedule = LearningRateScheduler(lambda epoch: initial_learning_rate * 0.9 ** epoch)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=initial_learning_rate),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJIH1AS4s6vF",
        "outputId": "95d77526-4467-4da9-ffe6-885db4c25e24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 577/4379 [==>...........................] - ETA: 25:48:20 - loss: 0.5019 - accuracy: 0.7506"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_dataset,\n",
        "          epochs=epochs,\n",
        "          validation_data=val_dataset,\n",
        "          callbacks=[lr_schedule])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6qturqAseqG"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.resnet.preprocess_input\n",
        ")\n",
        "\n",
        "test_dataset = test_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, \"Test\"),\n",
        "    #target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhy_Lt6VjZyL"
      },
      "source": [
        "分隔線 - ResNet 參數調整 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoBdhAqYjiU_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlNAgBLJjj-n"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBlr7fSyjlbC"
      },
      "outputs": [],
      "source": [
        "# Path to the directory containing the dataset\n",
        "data_dir = \"/content/drive/MyDrive/MIS 548 Porject Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp-sD0K8jpqx"
      },
      "outputs": [],
      "source": [
        "# Data sampling and loading\n",
        "def sample_data(directory, sample_size):\n",
        "    files = os.listdir(directory)\n",
        "    sampled_files = random.sample(files, sample_size)\n",
        "    return [os.path.join(directory, file) for file in sampled_files]\n",
        "\n",
        "def load_data(directory, sample_size):\n",
        "    real_samples = sample_data(os.path.join(directory, \"Real\"), sample_size // 2)\n",
        "    fake_samples = sample_data(os.path.join(directory, \"Fake\"), sample_size // 2)\n",
        "    all_samples = real_samples + fake_samples\n",
        "    labels = [1] * (sample_size // 2) + [0] * (sample_size // 2)\n",
        "    all_samples, labels = shuffle(all_samples, labels)\n",
        "    return all_samples, labels\n",
        "\n",
        "train_samples, train_labels = load_data(os.path.join(data_dir, \"Train\"), 10000)\n",
        "val_samples, val_labels = load_data(os.path.join(data_dir, \"Validation\"), 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne78_t01jsek",
        "outputId": "c689f644-5248-4550-fde8-67192932d948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 140110 images belonging to 2 classes.\n",
            "Found 39428 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing and augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.resnet.preprocess_input,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_dataset = train_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, \"Train\"),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.resnet.preprocess_input\n",
        ")\n",
        "\n",
        "val_dataset = val_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, \"Validation\"),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xchTt2tekBIL"
      },
      "outputs": [],
      "source": [
        "# Define the ResNet model architecture\n",
        "base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1hUZZ3EkCzW"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK7qdIrjkFW_"
      },
      "outputs": [],
      "source": [
        "# Compile the model with a custom learning rate scheduler and early stopping\n",
        "initial_learning_rate = 0.001\n",
        "epochs = 5\n",
        "\n",
        "lr_schedule = LearningRateScheduler(lambda epoch: initial_learning_rate * 0.9 ** epoch)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=initial_learning_rate),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "kkWPo5exkIRe",
        "outputId": "f7cdbcee-9932-4781-ff80-d4d9ce18f982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            " 673/4379 [===>..........................] - ETA: 4:01:38 - loss: 0.4915 - accuracy: 0.7550"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d95918042d88>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(train_dataset,\n\u001b[0m\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     callbacks=[lr_schedule, early_stopping])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=[lr_schedule, early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELcg2R35jbB6"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.resnet.preprocess_input\n",
        ")\n",
        "\n",
        "test_dataset = test_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, \"Test\"),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o4WZd8T3D8u"
      },
      "source": [
        "VGG 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbNewgBR-QFE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtxGFZJ0CxhN"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KypEIP8TCzCD"
      },
      "outputs": [],
      "source": [
        "# Path to the directory containing the dataset\n",
        "data_dir = \"/content/drive/MyDrive/MIS 548 Porject Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hABkZkw_C0SL"
      },
      "outputs": [],
      "source": [
        "# Data sampling and loading\n",
        "def sample_data(directory, sample_size):\n",
        "    files = os.listdir(directory)\n",
        "    sampled_files = random.sample(files, sample_size)\n",
        "    return [os.path.join(directory, file) for file in sampled_files]\n",
        "\n",
        "def load_data(directory, sample_size):\n",
        "    real_samples = sample_data(os.path.join(directory, \"Real\"), sample_size // 2)\n",
        "    fake_samples = sample_data(os.path.join(directory, \"Fake\"), sample_size // 2)\n",
        "    all_samples = real_samples + fake_samples\n",
        "    labels = [1] * (sample_size // 2) + [0] * (sample_size // 2)\n",
        "    all_samples, labels = shuffle(all_samples, labels)\n",
        "    return all_samples, labels\n",
        "\n",
        "train_samples, train_labels = load_data(os.path.join(data_dir, \"Train\"), 10000)\n",
        "val_samples, val_labels = load_data(os.path.join(data_dir, \"Validation\"), 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6dFmBqhC3jT"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "def preprocess_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = tf.keras.applications.vgg16.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    return preprocess_image(image), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yODodl77C5BT"
      },
      "outputs": [],
      "source": [
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_samples, train_labels))\n",
        "train_dataset = train_dataset.map(load_and_preprocess_image)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_samples)).batch(32)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_samples, val_labels))\n",
        "val_dataset = val_dataset.map(load_and_preprocess_image)\n",
        "val_dataset = val_dataset.batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIe3irzdC6oK"
      },
      "outputs": [],
      "source": [
        "# Define the VGG16 model architecture\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xpqr85o5C8Vo"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1Id8swVC-CG",
        "outputId": "0ffef68f-b5fe-4038-fcc7-45297dc40881"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 395s 726ms/step - loss: 1.1026 - accuracy: 0.5008 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 68s 142ms/step - loss: 0.6939 - accuracy: 0.5087 - val_loss: 0.6948 - val_accuracy: 0.5010\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 554s 2s/step - loss: 0.6922 - accuracy: 0.5232 - val_loss: 0.6930 - val_accuracy: 0.5080\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 60s 136ms/step - loss: 0.6917 - accuracy: 0.5262 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 60s 136ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7aa7379b94b0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_dataset,\n",
        "          epochs=5,\n",
        "          validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v63faEUWDG-p",
        "outputId": "da31098d-5905-4148-8cda-8b655089a956"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save(\"vgg16_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQQ12gwI3DcS",
        "outputId": "62fdebd3-84bc-4500-8890-9e1429d02564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 182s 3s/step - loss: 0.6932 - accuracy: 0.5000\n",
            "Test Loss: 0.6931784152984619\n",
            "Test Accuracy: 0.5\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "test_samples, test_labels = load_data(os.path.join(data_dir, \"Test\"), 2000)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_samples, test_labels))\n",
        "test_dataset = test_dataset.map(load_and_preprocess_image)\n",
        "test_dataset = test_dataset.batch(32)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htdHeNNiFOdS"
      },
      "source": [
        "ResNet 101\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdXwr_XGFbqC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet101\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oU9thuqFeaI"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCyIitwSFfuA"
      },
      "outputs": [],
      "source": [
        "# Path to the directory containing the dataset\n",
        "data_dir = \"/content/drive/MyDrive/MIS 548 Porject Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQv29L5nFkuN"
      },
      "outputs": [],
      "source": [
        "# Data sampling and loading\n",
        "def sample_data(directory, sample_size):\n",
        "    files = os.listdir(directory)\n",
        "    sampled_files = random.sample(files, sample_size)\n",
        "    return [os.path.join(directory, file) for file in sampled_files]\n",
        "\n",
        "def load_data(directory, sample_size):\n",
        "    real_samples = sample_data(os.path.join(directory, \"Real\"), sample_size // 2)\n",
        "    fake_samples = sample_data(os.path.join(directory, \"Fake\"), sample_size // 2)\n",
        "    all_samples = real_samples + fake_samples\n",
        "    labels = [1] * (sample_size // 2) + [0] * (sample_size // 2)\n",
        "    all_samples, labels = shuffle(all_samples, labels)\n",
        "    return all_samples, labels\n",
        "\n",
        "train_samples, train_labels = load_data(os.path.join(data_dir, \"Train\"), 10000)\n",
        "val_samples, val_labels = load_data(os.path.join(data_dir, \"Validation\"), 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GTA-_ujFmyt"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "def preprocess_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    return preprocess_image(image), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EibQA5p8FoR8"
      },
      "outputs": [],
      "source": [
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_samples, train_labels))\n",
        "train_dataset = train_dataset.map(load_and_preprocess_image)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_samples)).batch(32)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_samples, val_labels))\n",
        "val_dataset = val_dataset.map(load_and_preprocess_image)\n",
        "val_dataset = val_dataset.batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk62sIgEFp5p",
        "outputId": "999a4168-0eae-40a2-ce9c-eea57e736f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171446536/171446536 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Define the ResNet101 model architecture\n",
        "base_model = ResNet101(weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2yiCXEWFr-z"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukszdpz8FtmL",
        "outputId": "9cb87360-5886-4bc1-f21a-c4e59218287e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 2132s 7s/step - loss: 0.3446 - accuracy: 0.8598 - val_loss: 0.6468 - val_accuracy: 0.7580\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 79s 193ms/step - loss: 0.1697 - accuracy: 0.9307 - val_loss: 1.1761 - val_accuracy: 0.7610\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 78s 193ms/step - loss: 0.1277 - accuracy: 0.9484 - val_loss: 0.5023 - val_accuracy: 0.9075\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 78s 192ms/step - loss: 0.1086 - accuracy: 0.9577 - val_loss: 0.3187 - val_accuracy: 0.9015\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 78s 192ms/step - loss: 0.0946 - accuracy: 0.9623 - val_loss: 0.2026 - val_accuracy: 0.9300\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7aa684194ca0>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_dataset,\n",
        "          epochs=5,\n",
        "          validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2GNudCbF5_Y"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save(\"resnet101_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFujN4gkFMW0",
        "outputId": "4eb64839-6873-4fb3-f598-450c1487d8b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 70s 1s/step - loss: 0.3668 - accuracy: 0.8700\n",
            "Test Loss: 0.36679160594940186\n",
            "Test Accuracy: 0.8700000047683716\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "test_samples, test_labels = load_data(os.path.join(data_dir, \"Test\"), 2000)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_samples, test_labels))\n",
        "test_dataset = test_dataset.map(load_and_preprocess_image)\n",
        "test_dataset = test_dataset.batch(32)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BjPmGdWTIID"
      },
      "source": [
        "Inception V3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90dK2LyFTLSq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9iv3JHyTq65"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gPETbZcTsiN"
      },
      "outputs": [],
      "source": [
        "# Path to the directory containing the dataset\n",
        "data_dir = \"/content/drive/MyDrive/MIS 548 Porject Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcLxTl_ETxMV"
      },
      "outputs": [],
      "source": [
        "# Data sampling and loading\n",
        "def sample_data(directory, sample_size):\n",
        "    files = os.listdir(directory)\n",
        "    sampled_files = random.sample(files, sample_size)\n",
        "    return [os.path.join(directory, file) for file in sampled_files]\n",
        "\n",
        "def load_data(directory, sample_size):\n",
        "    real_samples = sample_data(os.path.join(directory, \"Real\"), sample_size // 2)\n",
        "    fake_samples = sample_data(os.path.join(directory, \"Fake\"), sample_size // 2)\n",
        "    all_samples = real_samples + fake_samples\n",
        "    labels = [1] * (sample_size // 2) + [0] * (sample_size // 2)\n",
        "    all_samples, labels = shuffle(all_samples, labels)\n",
        "    return all_samples, labels\n",
        "\n",
        "train_samples, train_labels = load_data(os.path.join(data_dir, \"Train\"), 10000)\n",
        "val_samples, val_labels = load_data(os.path.join(data_dir, \"Validation\"), 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-uCC34GTzpj"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "def preprocess_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (299, 299))  # InceptionV3 input size\n",
        "    image = tf.keras.applications.inception_v3.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    return preprocess_image(image), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFxNLhpXT14K"
      },
      "outputs": [],
      "source": [
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_samples, train_labels))\n",
        "train_dataset = train_dataset.map(load_and_preprocess_image)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_samples)).batch(32)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_samples, val_labels))\n",
        "val_dataset = val_dataset.map(load_and_preprocess_image)\n",
        "val_dataset = val_dataset.batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJjBS74qT3mR",
        "outputId": "21481bdf-41d8-45cb-8715-36344635ce49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 3s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Define the GoogLeNet (InceptionV3) model architecture\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiF_-wp4T7CR"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRh2iRYfT77h",
        "outputId": "757de21b-5e9a-40b3-bd02-fb6e0eed3e80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 948s 176ms/step - loss: 0.2010 - accuracy: 0.9127 - val_loss: 0.3440 - val_accuracy: 0.9010\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 68s 164ms/step - loss: 0.1058 - accuracy: 0.9575 - val_loss: 0.4894 - val_accuracy: 0.8080\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 67s 164ms/step - loss: 0.0823 - accuracy: 0.9674 - val_loss: 0.2343 - val_accuracy: 0.9115\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 68s 164ms/step - loss: 0.0736 - accuracy: 0.9709 - val_loss: 0.3942 - val_accuracy: 0.8550\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 68s 164ms/step - loss: 0.0504 - accuracy: 0.9789 - val_loss: 0.1590 - val_accuracy: 0.9510\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7aa684282980>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_dataset,\n",
        "          epochs=5,\n",
        "          validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCzzPU0rUAQf"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save(\"inceptionv3_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSH1HYa7THUQ",
        "outputId": "56a3dd43-13f2-40b8-b853-934695a196b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 68s 1s/step - loss: 0.4633 - accuracy: 0.8670\n",
            "Test Loss: 0.4633050262928009\n",
            "Test Accuracy: 0.8669999837875366\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "test_samples, test_labels = load_data(os.path.join(data_dir, \"Test\"), 2000)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_samples, test_labels))\n",
        "test_dataset = test_dataset.map(load_and_preprocess_image)\n",
        "test_dataset = test_dataset.batch(32)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhAxDtR4hD77"
      },
      "source": [
        "EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR9GtTy2hIra"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1aqzbVmhMKB"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsU30Ei6hNMW"
      },
      "outputs": [],
      "source": [
        "# Path to the directory containing the dataset\n",
        "data_dir = \"/content/drive/MyDrive/MIS 548 Porject Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYpqfWgKhSjX"
      },
      "outputs": [],
      "source": [
        "# Data sampling and loading\n",
        "def sample_data(directory, sample_size):\n",
        "    files = os.listdir(directory)\n",
        "    sampled_files = random.sample(files, sample_size)\n",
        "    return [os.path.join(directory, file) for file in sampled_files]\n",
        "\n",
        "def load_data(directory, sample_size):\n",
        "    real_samples = sample_data(os.path.join(directory, \"Real\"), sample_size // 2)\n",
        "    fake_samples = sample_data(os.path.join(directory, \"Fake\"), sample_size // 2)\n",
        "    all_samples = real_samples + fake_samples\n",
        "    labels = [1] * (sample_size // 2) + [0] * (sample_size // 2)\n",
        "    all_samples, labels = shuffle(all_samples, labels)\n",
        "    return all_samples, labels\n",
        "\n",
        "train_samples, train_labels = load_data(os.path.join(data_dir, \"Train\"), 10000)\n",
        "val_samples, val_labels = load_data(os.path.join(data_dir, \"Validation\"), 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X36jSZWqhVop"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "def preprocess_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (224, 224))  # EfficientNet input size\n",
        "    image = tf.keras.applications.efficientnet.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    return preprocess_image(image), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qGuawt3hYGB"
      },
      "outputs": [],
      "source": [
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_samples, train_labels))\n",
        "train_dataset = train_dataset.map(load_and_preprocess_image)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_samples)).batch(32)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_samples, val_labels))\n",
        "val_dataset = val_dataset.map(load_and_preprocess_image)\n",
        "val_dataset = val_dataset.batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTMDh03Ghbm-",
        "outputId": "8adb22d5-7913-4dcc-a72c-0fca60537a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Define the EfficientNet model architecture\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJv-nPMXhdDv"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7k6ts69heab",
        "outputId": "fd7ebc9b-39df-4f33-8472-9539db5ab537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 1080s 652ms/step - loss: 0.1629 - accuracy: 0.9345 - val_loss: 0.1394 - val_accuracy: 0.9490\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 51s 116ms/step - loss: 0.0657 - accuracy: 0.9720 - val_loss: 0.1614 - val_accuracy: 0.9405\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 51s 116ms/step - loss: 0.0483 - accuracy: 0.9806 - val_loss: 0.1565 - val_accuracy: 0.9480\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 52s 117ms/step - loss: 0.0414 - accuracy: 0.9840 - val_loss: 0.1289 - val_accuracy: 0.9575\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 52s 116ms/step - loss: 0.0352 - accuracy: 0.9867 - val_loss: 0.1778 - val_accuracy: 0.9405\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7be06e1f2ce0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_dataset,\n",
        "          epochs=5,\n",
        "          validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeVbxa8uhga7",
        "outputId": "7c04b25a-0968-4fba-aa43-82672ed8ebfc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save(\"efficientnet_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjr2Z-jshGPG",
        "outputId": "fcfef476-e3ea-4f83-a0b6-534b8da259ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 219s 3s/step - loss: 0.3738 - accuracy: 0.8810\n",
            "Test Loss: 0.37379613518714905\n",
            "Test Accuracy: 0.8809999823570251\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "test_samples, test_labels = load_data(os.path.join(data_dir, \"Test\"), 2000)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_samples, test_labels))\n",
        "test_dataset = test_dataset.map(load_and_preprocess_image)\n",
        "test_dataset = test_dataset.batch(32)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the provided output, here are some insights that can help identify which technique might be more suitable:\n",
        "\n",
        "**Training and Validation Performance:**\n",
        "\n",
        "The training accuracy starts at 93.45% and increases to 98.67% over the epochs.\n",
        "The validation accuracy starts at 94.90% and fluctuates slightly but remains relatively high, ranging between 94.05% and 95.75%.\n",
        "This indicates that the model is learning well from the training data and generalizing reasonably well to unseen validation data.\n",
        "\n",
        "**Test Performance:**\n",
        "\n",
        "The test accuracy is 88.10%, which is lower than the validation accuracy.\n",
        "This suggests that there might be some overfitting occurring, as the model is not performing as well on unseen test data as it did on the validation set.\n",
        "Given these insights, the following techniques may be suitable to improve test accuracy:\n",
        "\n",
        "* Regularization: Since the training accuracy is significantly higher than the test accuracy, it indicates that the model might be overfitting. Regularization techniques such as dropout or weight decay can help reduce overfitting and improve generalization to the test set.\n",
        "* Data Augmentation: If the validation accuracy remains stable but the test accuracy is significantly lower, it might suggest that the model is not exposed to enough variations in the training data. Data augmentation techniques can help introduce more diversity into the training data, potentially leading to better generalization to unseen test data.\n",
        "* Learning Rate Scheduling: If the training and validation accuracy plateaus or fluctuates significantly, adjusting the learning rate schedule may help stabilize the training process and improve convergence, which can indirectly lead to better test accuracy.\n",
        "\n",
        "Given these insights, a combination of regularization techniques and data augmentation may be particularly effective in improving test accuracy while maintaining the same number of sampling data."
      ],
      "metadata": {
        "id": "Gl-Yp1GpvPV-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LRWT2-Bv5un"
      },
      "source": [
        "EfficientNetB1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaud6AE-v_DT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB1  # Change import statement\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9HsPeT8wDYI"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOzafOP2wEs6"
      },
      "outputs": [],
      "source": [
        "# Path to the directory containing the dataset\n",
        "data_dir = \"/content/drive/MyDrive/MIS 548 Porject Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym4KKJESwJIM"
      },
      "outputs": [],
      "source": [
        "# Data sampling and loading\n",
        "def sample_data(directory, sample_size):\n",
        "    files = os.listdir(directory)\n",
        "    sampled_files = random.sample(files, sample_size)\n",
        "    return [os.path.join(directory, file) for file in sampled_files]\n",
        "\n",
        "def load_data(directory, sample_size):\n",
        "    real_samples = sample_data(os.path.join(directory, \"Real\"), sample_size // 2)\n",
        "    fake_samples = sample_data(os.path.join(directory, \"Fake\"), sample_size // 2)\n",
        "    all_samples = real_samples + fake_samples\n",
        "    labels = [1] * (sample_size // 2) + [0] * (sample_size // 2)\n",
        "    all_samples, labels = shuffle(all_samples, labels)\n",
        "    return all_samples, labels\n",
        "\n",
        "train_samples, train_labels = load_data(os.path.join(data_dir, \"Train\"), 10000)\n",
        "val_samples, val_labels = load_data(os.path.join(data_dir, \"Validation\"), 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJMEemKowLBu"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "def preprocess_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (224, 224))  # EfficientNet input size\n",
        "    image = tf.keras.applications.efficientnet.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    return preprocess_image(image), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LTmi-_pwM8c"
      },
      "outputs": [],
      "source": [
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_samples, train_labels))\n",
        "train_dataset = train_dataset.map(load_and_preprocess_image)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_samples)).batch(32)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_samples, val_labels))\n",
        "val_dataset = val_dataset.map(load_and_preprocess_image)\n",
        "val_dataset = val_dataset.batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HMi1FL8wOcM",
        "outputId": "ef2d2967-736b-44ee-b43e-b5bc80e9e641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
            "27018416/27018416 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Define the EfficientNetB1 model architecture\n",
        "base_model = EfficientNetB1(weights='imagenet', include_top=False)  # Change model architecture\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tuuj9UB2wQFk"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCRLiARcwSE9",
        "outputId": "bac37444-e019-4001-ffb5-3e4e7474605e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 677s 2s/step - loss: 0.1659 - accuracy: 0.9365 - val_loss: 0.2945 - val_accuracy: 0.8950\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 62s 152ms/step - loss: 0.0750 - accuracy: 0.9716 - val_loss: 0.1333 - val_accuracy: 0.9570\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 62s 153ms/step - loss: 0.0506 - accuracy: 0.9801 - val_loss: 0.1972 - val_accuracy: 0.9345\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 62s 153ms/step - loss: 0.0379 - accuracy: 0.9838 - val_loss: 0.1561 - val_accuracy: 0.9450\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 62s 154ms/step - loss: 0.0370 - accuracy: 0.9860 - val_loss: 0.2246 - val_accuracy: 0.9400\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d1cdc692320>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_dataset,\n",
        "          epochs=5,\n",
        "          validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smoJOmzUwU3P",
        "outputId": "6741642c-c116-4707-c109-638ffd1f9fbd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save(\"efficientnetB1_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sO2ire-_v9lY",
        "outputId": "ca0627ac-092b-4e97-8536-3fd8677c4757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 179s 3s/step - loss: 0.8707 - accuracy: 0.8550\n",
            "Test Loss: 0.8706620335578918\n",
            "Test Accuracy: 0.8550000190734863\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "test_samples, test_labels = load_data(os.path.join(data_dir, \"Test\"), 2000)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_samples, test_labels))\n",
        "test_dataset = test_dataset.map(load_and_preprocess_image)\n",
        "test_dataset = test_dataset.batch(32)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cZPGUEX8Op8"
      },
      "source": [
        "EfficientNetB2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_hBBOAD8R04"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB2  # Change import statement\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDH6zqin8TW2"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdugcgoj8Xq3"
      },
      "outputs": [],
      "source": [
        "# Path to the directory containing the dataset\n",
        "data_dir = \"/content/drive/MyDrive/MIS 548 Porject Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEJcl0_P8YXj"
      },
      "outputs": [],
      "source": [
        "# Data sampling and loading\n",
        "def sample_data(directory, sample_size):\n",
        "    files = os.listdir(directory)\n",
        "    sampled_files = random.sample(files, sample_size)\n",
        "    return [os.path.join(directory, file) for file in sampled_files]\n",
        "\n",
        "def load_data(directory, sample_size):\n",
        "    real_samples = sample_data(os.path.join(directory, \"Real\"), sample_size // 2)\n",
        "    fake_samples = sample_data(os.path.join(directory, \"Fake\"), sample_size // 2)\n",
        "    all_samples = real_samples + fake_samples\n",
        "    labels = [1] * (sample_size // 2) + [0] * (sample_size // 2)\n",
        "    all_samples, labels = shuffle(all_samples, labels)\n",
        "    return all_samples, labels\n",
        "\n",
        "train_samples, train_labels = load_data(os.path.join(data_dir, \"Train\"), 10000)\n",
        "val_samples, val_labels = load_data(os.path.join(data_dir, \"Validation\"), 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMfiBK348a6Y"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "def preprocess_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = tf.keras.applications.efficientnet.preprocess_input(image)  # Change preprocessing function\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    return preprocess_image(image), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIaE1n8w8dOj"
      },
      "outputs": [],
      "source": [
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_samples, train_labels))\n",
        "train_dataset = train_dataset.map(load_and_preprocess_image)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_samples)).batch(32)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_samples, val_labels))\n",
        "val_dataset = val_dataset.map(load_and_preprocess_image)\n",
        "val_dataset = val_dataset.batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vLC0ynt8fE1",
        "outputId": "6a63cd21-f83b-454c-e6c9-c44b45ca9ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
            "31790344/31790344 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Define the EfficientNetB2 model architecture\n",
        "base_model = EfficientNetB2(weights='imagenet', include_top=False)  # Change model architecture\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AoTy_eu8iR8"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmgxy0KQ8jWQ",
        "outputId": "ce30e077-134d-497f-afd7-d6e66b656601"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 1731s 165ms/step - loss: 0.1651 - accuracy: 0.9345 - val_loss: 0.2171 - val_accuracy: 0.9300\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 64s 158ms/step - loss: 0.0732 - accuracy: 0.9706 - val_loss: 0.1406 - val_accuracy: 0.9465\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 64s 159ms/step - loss: 0.0511 - accuracy: 0.9804 - val_loss: 0.1395 - val_accuracy: 0.9485\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 64s 158ms/step - loss: 0.0377 - accuracy: 0.9851 - val_loss: 0.1543 - val_accuracy: 0.9535\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 64s 158ms/step - loss: 0.0272 - accuracy: 0.9902 - val_loss: 0.2365 - val_accuracy: 0.9295\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d1bb3e112a0>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_dataset,\n",
        "          epochs=5,\n",
        "          validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNQRfTQo8lHb"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save(\"efficientnet_b2_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4B0aOXD8QxN",
        "outputId": "757c3ab7-0721-45f3-8597-ee8403909fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 64s 1s/step - loss: 0.3937 - accuracy: 0.8770\n",
            "Test Loss: 0.393665075302124\n",
            "Test Accuracy: 0.8769999742507935\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "test_samples, test_labels = load_data(os.path.join(data_dir, \"Test\"), 2000)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_samples, test_labels))\n",
        "test_dataset = test_dataset.map(load_and_preprocess_image)\n",
        "test_dataset = test_dataset.batch(32)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnhNfYJ3MkIi"
      },
      "source": [
        "EfficientNetB3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AvYZTbYMqUY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB3  # Update import statement\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VyoHFnjMrHc"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi3ixpxLMsxy"
      },
      "outputs": [],
      "source": [
        "# Path to the directory containing the dataset\n",
        "data_dir = \"/content/drive/MyDrive/MIS 548 Porject Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeHijntyMw4a"
      },
      "outputs": [],
      "source": [
        "# Data sampling and loading\n",
        "def sample_data(directory, sample_size):\n",
        "    files = os.listdir(directory)\n",
        "    sampled_files = random.sample(files, sample_size)\n",
        "    return [os.path.join(directory, file) for file in sampled_files]\n",
        "\n",
        "def load_data(directory, sample_size):\n",
        "    real_samples = sample_data(os.path.join(directory, \"Real\"), sample_size // 2)\n",
        "    fake_samples = sample_data(os.path.join(directory, \"Fake\"), sample_size // 2)\n",
        "    all_samples = real_samples + fake_samples\n",
        "    labels = [1] * (sample_size // 2) + [0] * (sample_size // 2)\n",
        "    all_samples, labels = shuffle(all_samples, labels)\n",
        "    return all_samples, labels\n",
        "\n",
        "train_samples, train_labels = load_data(os.path.join(data_dir, \"Train\"), 10000)\n",
        "val_samples, val_labels = load_data(os.path.join(data_dir, \"Validation\"), 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o77yD1T2M5BN"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "def preprocess_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = tf.keras.applications.efficientnet.preprocess_input(image)  # Update preprocessing function\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    return preprocess_image(image), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWzOMrp_M61D"
      },
      "outputs": [],
      "source": [
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_samples, train_labels))\n",
        "train_dataset = train_dataset.map(load_and_preprocess_image)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_samples)).batch(32)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_samples, val_labels))\n",
        "val_dataset = val_dataset.map(load_and_preprocess_image)\n",
        "val_dataset = val_dataset.batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsgpSORyM8iU"
      },
      "outputs": [],
      "source": [
        "# Define the EfficientNetB3 model architecture\n",
        "base_model = EfficientNetB3(weights='imagenet', include_top=False)  # Update model architecture\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Fs00viWM-XS"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4Zaoce-NA6n",
        "outputId": "0b6c0df9-6d80-4beb-ed6e-64a98504200b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            " 95/313 [========>.....................] - ETA: 1:49 - loss: 0.2752 - accuracy: 0.8888"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_dataset,\n",
        "          epochs=5,\n",
        "          validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFt_VMTrNCXf"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save(\"efficientnet_b3_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHDZymf5Mmel"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "test_samples, test_labels = load_data(os.path.join(data_dir, \"Test\"), 2000)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_samples, test_labels))\n",
        "test_dataset = test_dataset.map(load_and_preprocess_image)\n",
        "test_dataset = test_dataset.batch(32)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}